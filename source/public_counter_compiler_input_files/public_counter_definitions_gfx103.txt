;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
;; Copyright (c) 2020-2024 Advanced Micro Devices, Inc. All rights reserved.
;;
;; Counter definitions for DX/VK/GL for Gfx103 (GFX IP v10.3)
;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;

name=GPUTime
desc=#Timing#Time this API command took to execute on the GPU in nanoseconds from the time the previous command reached the bottom of the pipeline (BOP) to the time this command reaches the bottom of the pipeline (BOP). Does not include time that draw calls are processed in parallel.
type=gpa_float64
usage=nanoseconds
spm=0
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
GPUTime_Bottom_To_Bottom_Duration
eqn=GPUTime_Bottom_To_Bottom_Duration,TS_FREQ,/,(1000000000),*

name=ExecutionDuration
desc=#Timing#GPU command execution duration in nanoseconds, from the time the command enters the top of the pipeline (TOP) to the time the command reaches the bottom of the pipeline (BOP). Does not include time that draw calls are processed in parallel.
type=gpa_float64
usage=nanoseconds
spm=0
[DX12Gfx103]
;[OGLPGfx103]
[VKGfx103]
GPUTime_Top_To_Bottom_Duration
eqn=GPUTime_Top_To_Bottom_Duration,TS_FREQ,/,(1000000000),*

name=ExecutionStart
desc=#Timing#GPU command execution start time in nanoseconds. This is the time the command enters the top of the pipeline (TOP).
type=gpa_float64
usage=nanoseconds
spm=0
[DX12Gfx103]
;[OGLPGfx103]
[VKGfx103]
GPUTime_Top_To_Bottom_Start
eqn=GPUTime_Top_To_Bottom_Start,TS_FREQ,/,(1000000000),*

name=ExecutionEnd
desc=#Timing#GPU command execution end time in nanoseconds. This is the time the command reaches the bottom of the pipeline (BOP).
type=gpa_float64
usage=nanoseconds
spm=0
[DX12Gfx103]
;[OGLPGfx103]
[VKGfx103]
GPUTime_Top_To_Bottom_End
eqn=GPUTime_Top_To_Bottom_End,TS_FREQ,/,(1000000000),*

name=GPUBusy
usage=percentage
desc=#Timing#The percentage of time the GPU command processor was busy.
type=gpa_float64
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
CPF_PERF_SEL_CPF_STAT_BUSY
CPF_PERF_SEL_ALWAYS_COUNT
eqn=0,1,/,(100),*,(100),min

name=GPUBusyCycles
usage=cycles
desc=#Timing#Number of GPU cycles that the GPU command processor was busy.
type=gpa_float64
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
CPF_PERF_SEL_CPF_STAT_BUSY
eqn=0

name=TessellatorBusy
usage=percentage
desc=#Timing#The percentage of time the tessellation engine is busy.
type=gpa_float64
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
GE2_DIST_GE_DIST_WD_TE11_BUSY
CPF_PERF_SEL_CPF_STAT_BUSY
eqn=0,1,/,(100),*

name=TessellatorBusyCycles
usage=cycles
desc=#Timing#Number of GPU cycles that the tessellation engine is busy.
type=gpa_float64
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
GE2_DIST_GE_DIST_WD_TE11_BUSY
eqn=0

name=VsGsBusy
desc=#Timing#The percentage of time the ShaderUnit has VS or GS work to do in a VS-[GS-]PS pipeline.
type=gpa_float64
usage=percentage
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
SPI*_PERF_VS_BUSY[0..3]
SPI*_PERF_GS_BUSY[0..3]
SPI*_PERF_HS_WAVE[0..3]
CPF_PERF_SEL_CPF_STAT_BUSY
eqn=0..7,max8,(0),8..11,sum4,ifnotzero,12,/,(100),*,(100),min

name=VsGsBusyCycles
desc=#Timing#Number of GPU cycles that the ShaderUnit has VS or GS work to do in a VS-[GS-]PS pipeline.
type=gpa_float64
usage=cycles
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
SPI*_PERF_VS_BUSY[0..3]
SPI*_PERF_GS_BUSY[0..3]
SPI*_PERF_HS_WAVE[0..3]
eqn=0..7,max8,(0),8..11,sum4,ifnotzero

name=VsGsTime
desc=#Timing#Time VS or GS are busy in nanoseconds in a VS-[GS-]PS pipeline.
type=gpa_float64
usage=nanoseconds
spm=0
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
GPUTime_Bottom_To_Bottom_Duration
SPI*_PERF_VS_BUSY[0..3]
SPI*_PERF_GS_BUSY[0..3]
SPI*_PERF_HS_WAVE[0..3]
CPF_PERF_SEL_CPF_STAT_BUSY
eqn=1..8,max8,(0),9..12,sum4,ifnotzero,13,/,(1),min,GPUTime_Bottom_To_Bottom_Duration,TS_FREQ,/,(1000000000),*,*

name=PreTessellationBusy
desc=#Timing#The percentage of time the ShaderUnit has VS and HS work to do in a pipeline that uses tessellation.
type=gpa_float64
usage=percentage
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
SPI*_PERF_HS_BUSY[0..3]
SPI*_PERF_HS_WAVE[0..3]
CPF_PERF_SEL_CPF_STAT_BUSY
eqn=(0),0..3,max4,4..7,sum4,ifnotzero,8,/,(100),*,(100),min

name=PreTessellationBusyCycles
desc=#Timing#Number of GPU cycles that the ShaderUnit has VS and HS work to do in a pipeline that uses tessellation.
type=gpa_float64
usage=cycles
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
SPI*_PERF_HS_BUSY[0..3]
SPI*_PERF_HS_WAVE[0..3]
eqn=(0),0..3,max4,4..7,sum4,ifnotzero

name=PreTessellationTime
desc=#Timing#Time VS and HS are busy in nanoseconds in a pipeline that uses tessellation.
type=gpa_float64
usage=nanoseconds
spm=0
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
GPUTime_Bottom_To_Bottom_Duration
SPI*_PERF_HS_BUSY[0..3]
SPI*_PERF_HS_WAVE[0..3]
CPF_PERF_SEL_CPF_STAT_BUSY
eqn=(0),1..4,max4,5..8,sum4,ifnotzero,9,/,(1),min,GPUTime_Bottom_To_Bottom_Duration,TS_FREQ,/,(1000000000),*,*

name=PostTessellationBusy
desc=#Timing#The percentage of time the ShaderUnit has DS or GS work to do in a pipeline that uses tessellation.
type=gpa_float64
usage=percentage
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
SPI*_PERF_VS_BUSY[0..3]
SPI*_PERF_GS_BUSY[0..3]
SPI*_PERF_HS_WAVE[0..3]
CPF_PERF_SEL_CPF_STAT_BUSY
eqn=(0),0..7,max8,8..11,sum4,ifnotzero,12,/,(100),*,(100),min

name=PostTessellationBusyCycles
desc=#Timing#Number of GPU cycles that the ShaderUnit has DS or GS work to do in a pipeline that uses tessellation.
type=gpa_float64
usage=cycles
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
SPI*_PERF_VS_BUSY[0..3]
SPI*_PERF_GS_BUSY[0..3]
SPI*_PERF_HS_WAVE[0..3]
eqn=(0),0..7,max8,8..11,sum4,ifnotzero

name=PostTessellationTime
desc=#Timing#Time DS or GS are busy in nanoseconds in a pipeline that uses tessellation.
type=gpa_float64
usage=nanoseconds
spm=0
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
GPUTime_Bottom_To_Bottom_Duration
SPI*_PERF_VS_BUSY[0..3]
SPI*_PERF_GS_BUSY[0..3]
SPI*_PERF_HS_WAVE[0..3]
CPF_PERF_SEL_CPF_STAT_BUSY
eqn=(0),1..8,max8,9..12,sum4,ifnotzero,13,/,(1),min,GPUTime_Bottom_To_Bottom_Duration,TS_FREQ,/,(1000000000),*,*

name=PSBusy
desc=#Timing#The percentage of time the ShaderUnit has pixel shader work to do.
type=gpa_float64
usage=percentage
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
SPI*_PERF_PS0_BUSY[0..3]
SPI*_PERF_PS0_WAVE[0..3]
SPI*_PERF_PS1_BUSY[0..3]
SPI*_PERF_PS1_WAVE[0..3]
SPI*_PERF_PS2_BUSY[0..3]
SPI*_PERF_PS2_WAVE[0..3]
SPI*_PERF_PS3_BUSY[0..3]
SPI*_PERF_PS3_WAVE[0..3]
CPF_PERF_SEL_CPF_STAT_BUSY
eqn=(0),0,4,ifnotzero,(0),1,5,ifnotzero,(0),2,6,ifnotzero,(0),3,7,ifnotzero,max4,(0),8,12,ifnotzero,(0),9,13,ifnotzero,(0),10,14,ifnotzero,(0),11,15,ifnotzero,max4,(0),16,20,ifnotzero,(0),17,21,ifnotzero,(0),18,22,ifnotzero,(0),19,23,ifnotzero,max4,(0),24,28,ifnotzero,(0),25,29,ifnotzero,(0),26,30,ifnotzero,(0),27,31,ifnotzero,max4,max4,32,/,(100),*,(100),min

name=PSBusyCycles
desc=#Timing#Number of GPU cycles that the ShaderUnit has pixel shader work to do.
type=gpa_float64
usage=cycles
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
SPI*_PERF_PS0_BUSY[0..3]
SPI*_PERF_PS0_WAVE[0..3]
SPI*_PERF_PS1_BUSY[0..3]
SPI*_PERF_PS1_WAVE[0..3]
SPI*_PERF_PS2_BUSY[0..3]
SPI*_PERF_PS2_WAVE[0..3]
SPI*_PERF_PS3_BUSY[0..3]
SPI*_PERF_PS3_WAVE[0..3]
eqn=(0),0,4,ifnotzero,(0),1,5,ifnotzero,(0),2,6,ifnotzero,(0),3,7,ifnotzero,max4,(0),8,12,ifnotzero,(0),9,13,ifnotzero,(0),10,14,ifnotzero,(0),11,15,ifnotzero,max4,(0),16,20,ifnotzero,(0),17,21,ifnotzero,(0),18,22,ifnotzero,(0),19,23,ifnotzero,max4,(0),24,28,ifnotzero,(0),25,29,ifnotzero,(0),26,30,ifnotzero,(0),27,31,ifnotzero,max4,max4

name=PSTime
desc=#Timing#Time pixel shaders are busy in nanoseconds.
type=gpa_float64
usage=nanoseconds
spm=0
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
GPUTime_Bottom_To_Bottom_Duration
SPI*_PERF_PS0_BUSY[0..3]
SPI*_PERF_PS0_WAVE[0..3]
SPI*_PERF_PS1_BUSY[0..3]
SPI*_PERF_PS1_WAVE[0..3]
SPI*_PERF_PS2_BUSY[0..3]
SPI*_PERF_PS2_WAVE[0..3]
SPI*_PERF_PS3_BUSY[0..3]
SPI*_PERF_PS3_WAVE[0..3]
CPF_PERF_SEL_CPF_STAT_BUSY
eqn=(0),1,5,ifnotzero,(0),2,6,ifnotzero,(0),3,7,ifnotzero,(0),4,8,ifnotzero,max4,(0),9,13,ifnotzero,(0),10,14,ifnotzero,(0),11,15,ifnotzero,(0),12,16,ifnotzero,max4,(0),17,21,ifnotzero,(0),18,22,ifnotzero,(0),19,23,ifnotzero,(0),20,24,ifnotzero,max4,(0),25,29,ifnotzero,(0),26,30,ifnotzero,(0),27,31,ifnotzero,(0),28,32,ifnotzero,max4,max4,33,/,(1),min,GPUTime_Bottom_To_Bottom_Duration,TS_FREQ,/,(1000000000),*,*

name=CSBusy
desc=#Timing#The percentage of time the ShaderUnit has compute shader work to do.
type=gpa_float64
usage=percentage
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
SPI*_PERF_CSGN_BUSY[0..3]
SPI*_PERF_CSGN_WAVE[0..3]
SPI*_PERF_CSN_BUSY[0..3]
SPI*_PERF_CSN_WAVE[0..3]
CPF_PERF_SEL_CPF_STAT_BUSY
eqn=SPI*_PERF_CSGN_BUSY,SPI*_PERF_CSGN_WAVE,comparemax4,SPI*_PERF_CSN_BUSY,SPI*_PERF_CSN_WAVE,comparemax4,max,CPF_PERF_SEL_CPF_STAT_BUSY,/,(100),*,(100),min

name=CSBusyCycles
desc=#Timing#Number of GPU cycles that the ShaderUnit has compute shader work to do.
type=gpa_float64
usage=cycles
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
SPI*_PERF_CSGN_BUSY[0..3]
SPI*_PERF_CSGN_WAVE[0..3]
SPI*_PERF_CSN_BUSY[0..3]
SPI*_PERF_CSN_WAVE[0..3]
eqn=SPI*_PERF_CSGN_BUSY,SPI*_PERF_CSGN_WAVE,comparemax4,SPI*_PERF_CSN_BUSY,SPI*_PERF_CSN_WAVE,comparemax4,max

name=CSTime
desc=#Timing#Time compute shaders are busy in nanoseconds.
type=gpa_float64
usage=nanoseconds
spm=0
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
GPUTime_Bottom_To_Bottom_Duration
SPI*_PERF_CSGN_BUSY[0..3]
SPI*_PERF_CSGN_WAVE[0..3]
SPI*_PERF_CSN_BUSY[0..3]
SPI*_PERF_CSN_WAVE[0..3]
CPF_PERF_SEL_CPF_STAT_BUSY
eqn=SPI*_PERF_CSGN_BUSY,SPI*_PERF_CSGN_WAVE,comparemax4,SPI*_PERF_CSN_BUSY,SPI*_PERF_CSN_WAVE,comparemax4,max,CPF_PERF_SEL_CPF_STAT_BUSY,/,(1),min,GPUTime_Bottom_To_Bottom_Duration,TS_FREQ,/,(1000000000),*,*

name=GSVerticesOut
desc=#VertexGeometry#The number of vertices output by the GS.
type=gpa_float64
usage=items
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
GE_SE*_SPI_VSVERT_VALID[0..3]
GE_SE*_SPI_ESVERT_VALID[0..3]
SPI*_PERF_HS_WAVE[0..3]
eqn=(0),0..3,sum4,4..7,sum4,ifnotzero,(0),8..11,sum4,ifnotzero

Name=VsGsVALUInstCount
desc=#VertexGeometry#Average number of vector ALU instructions executed for the VS and GS in a VS-[GS-]PS pipeline. Affected by flow control.
type=gpa_float64
usage=items
spm=0
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
SQ_VS*_PERF_SEL_INSTS_VALU[0..3]
SQ_VS*_PERF_SEL_WAVES[0..3]
SQ_GS*_PERF_SEL_INSTS_VALU[0..3]
SQ_GS*_PERF_SEL_WAVES[0..3]
SPI*_PERF_HS_WAVE[0..3]
eqn=(0),0..3,sum4,4..7,sum4,/,4..7,sum4,ifnotzero,8..11,sum4,12..15,sum4,/,12..15,sum4,ifnotzero,(0),SPI*_PERF_HS_WAVE,sum4,ifnotzero

name=VsGsSALUInstCount
desc=#VertexGeometry#Average number of scalar ALU instructions executed for the VS and GS. Affected by flow control.
type=gpa_float64
usage=items
spm=0
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
SQ_VS*_PERF_SEL_INSTS_SALU[0..3]
SQ_VS*_PERF_SEL_WAVES[0..3]
SQ_GS*_PERF_SEL_INSTS_SALU[0..3]
SQ_GS*_PERF_SEL_WAVES[0..3]
SPI*_PERF_HS_WAVE[0..3]
eqn=(0),0..3,sum4,4..7,sum4,/,4..7,sum4,ifnotzero,8..11,sum4,12..15,sum4,/,12..15,sum4,ifnotzero,(0),SPI*_PERF_HS_WAVE,sum4,ifnotzero

name=VsGsVALUBusy
desc=#VertexGeometry#The percentage of GPUTime vector ALU instructions are being processed for the VS and GS.
type=gpa_float64
usage=percentage
spm=0
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
SQ_VS*_PERF_SEL_INST_CYCLES_VALU[0..3]
SQ_GS*_PERF_SEL_INST_CYCLES_VALU[0..3]
CPF_PERF_SEL_CPF_STAT_BUSY
SPI*_PERF_HS_WAVE[0..3]
eqn=0..3,sum4,4..7,sum4,+,NUM_SIMDS,/,8,/,(100),*,(0),SPI*_PERF_HS_WAVE,sum4,ifnotzero

name=VsGsVALUBusyCycles
desc=#VertexGeometry#Number of GPU cycles where vector ALU instructions are being processed for the VS and GS.
type=gpa_float64
usage=cycles
spm=0
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
SQ_VS*_PERF_SEL_INST_CYCLES_VALU[0..3]
SQ_GS*_PERF_SEL_INST_CYCLES_VALU[0..3]
SPI*_PERF_HS_WAVE[0..3]
eqn=0..3,sum4,4..7,sum4,+,NUM_SIMDS,/,(0),SPI*_PERF_HS_WAVE,sum4,ifnotzero

name=VsGsSALUBusy
desc=#VertexGeometry#The percentage of GPUTime scalar ALU instructions are being processed for the VS and GS.
type=gpa_float64
usage=percentage
spm=0
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
SQ_VS*_PERF_SEL_INSTS_SALU[0..3]
SQ_GS*_PERF_SEL_INSTS_SALU[0..3]
CPF_PERF_SEL_CPF_STAT_BUSY
SPI*_PERF_HS_WAVE[0..3]
eqn=0..3,sum4,4..7,sum4,+,NUM_CUS,/,8,/,(100),*,(0),SPI*_PERF_HS_WAVE,sum4,ifnotzero

name=VsGsSALUBusyCycles
desc=#VertexGeometry#Number of GPU cycles where scalar ALU instructions are being processed for the VS and GS.
type=gpa_float64
usage=cycles
spm=0
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
SQ_VS*_PERF_SEL_INSTS_SALU[0..3]
SQ_GS*_PERF_SEL_INSTS_SALU[0..3]
SPI*_PERF_HS_WAVE[0..3]
eqn=0..3,sum4,4..7,sum4,+,NUM_CUS,/,(0),SPI*_PERF_HS_WAVE,sum4,ifnotzero


name=PreTessVALUInstCount
desc=#PreTessellation#Average number of vector ALU instructions executed for the VS and HS in a pipeline that uses tessellation. Affected by flow control.
type=gpa_float64
usage=items
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
SQ_HS*_PERF_SEL_INSTS_VALU[0..3]
SPI*_PERF_HS_WAVE[0..3]
eqn=(0),0..3,sum4,4..7,sum4,/,4..7,sum4,ifnotzero

name=PreTessSALUInstCount
desc=#PreTessellation#Average number of scalar ALU instructions executed for the VS and HS in a pipeline that uses tessellation. Affected by flow control.
type=gpa_float64
usage=items
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
SQ_HS*_PERF_SEL_INSTS_SALU[0..3]
SPI*_PERF_HS_WAVE[0..3]
eqn=(0),0..3,sum4,4..7,sum4,/,4..7,sum4,ifnotzero

name=PreTessVALUBusy
desc=#PreTessellation#The percentage of GPUTime vector ALU instructions are being processed for the VS and HS in a pipeline that uses tessellation.
type=gpa_float64
usage=percentage
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
SQ_HS*_PERF_SEL_INST_CYCLES_VALU[0..3]
SPI*_PERF_HS_WAVE[0..3]
CPF_PERF_SEL_CPF_STAT_BUSY
eqn=(0),0..3,sum4,NUM_SIMDS,/,8,/,(100),*,4..7,sum4,ifnotzero

name=PreTessVALUBusyCycles
desc=#PreTessellation#Number of GPU cycles vector where ALU instructions are being processed for the VS and HS in a pipeline that uses tessellation.
type=gpa_float64
usage=cycles
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
SQ_HS*_PERF_SEL_INST_CYCLES_VALU[0..3]
SPI*_PERF_HS_WAVE[0..3]
eqn=(0),0..3,sum4,NUM_SIMDS,/,4..7,sum4,ifnotzero

name=PreTessSALUBusy
desc=#PreTessellation#The percentage of GPUTime scalar ALU instructions are being processed for the VS and HS in a pipeline that uses tessellation.
type=gpa_float64
usage=percentage
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
SQ_HS*_PERF_SEL_INSTS_SALU[0..3]
SPI*_PERF_HS_WAVE[0..3]
CPF_PERF_SEL_CPF_STAT_BUSY
eqn=(0),0..3,sum4,NUM_CUS,/,8,/,(100),*,4..7,sum4,ifnotzero

name=PreTessSALUBusyCycles
desc=#PreTessellation#Number of GPU cycles where scalar ALU instructions are being processed for the VS and HS in a pipeline that uses tessellation.
type=gpa_float64
usage=cycles
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
SQ_HS*_PERF_SEL_INSTS_SALU[0..3]
SPI*_PERF_HS_WAVE[0..3]
eqn=(0),0..3,sum4,NUM_CUS,/,4..7,sum4,ifnotzero

name=PreTessVerticesIn
desc=#PreTessellation#The number of vertices processed by the VS and HS when using tessellation.
type=gpa_float64
usage=items
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
GE_SE*_SPI_HSVERT_VALID[0..3]
SPI*_PERF_HS_WAVE[0..3]
eqn=(0),0..3,sum4,4..7,sum4,ifnotzero

name=PostTessPrimsOut
desc=#PostTessellation#The number of primitives output by the DS and GS when using tessellation.
type=gpa_float64
usage=items
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
GE_SE*_DS_PRIMS[0..3]
SPI*_PERF_HS_WAVE[0..3]
eqn=(0),0..3,sum4,4..7,sum4,ifnotzero


name=PostTessVALUInstCount
desc=#PostTessellation#Average number of vector ALU instructions executed for the DS and GS in a pipeline that uses tessellation. Affected by flow control.
type=gpa_float64
usage=items
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
SQ_GS*_PERF_SEL_INSTS_VALU[0..3]
SQ_GS*_PERF_SEL_WAVES[0..3]
SPI*_PERF_HS_WAVE[0..3]
eqn=(0),0..3,sum4,4..7,sum4,/,SPI*_PERF_HS_WAVE,sum4,ifnotzero

name=PostTessSALUInstCount
desc=#PostTessellation#Average number of scalar ALU instructions executed for the DS and GS in a pipeline that uses tessellation. Affected by flow control.
type=gpa_float64
usage=items
spm=0
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
SQ_GS*_PERF_SEL_INSTS_SALU[0..3]
SQ_GS*_PERF_SEL_WAVES[0..3]
SPI*_PERF_HS_WAVE[0..3]
eqn=(0),0..3,sum4,4..7,sum4,/,8..11,sum4,ifnotzero

name=PostTessVALUBusy
desc=#PostTessellation#The percentage of GPUTime vector ALU instructions are being processed for the DS and GS in a pipeline that uses tessellation.
type=gpa_float64
usage=percentage
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
SQ_GS*_PERF_SEL_INST_CYCLES_VALU[0..3]
SPI*_PERF_HS_WAVE[0..3]
CPF_PERF_SEL_CPF_STAT_BUSY
eqn=(0),0..3,sum4,NUM_SIMDS,/,8,/,(100),*,4..7,sum4,ifnotzero

name=PostTessVALUBusyCycles
desc=#PostTessellation#Number of GPU cycles vector where ALU instructions are being processed for the DS and GS in a pipeline that uses tessellation.
type=gpa_float64
usage=cycles
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
SQ_GS*_PERF_SEL_INST_CYCLES_VALU[0..3]
SPI*_PERF_HS_WAVE[0..3]
eqn=(0),0..3,sum4,NUM_SIMDS,/,4..7,sum4,ifnotzero

name=PostTessSALUBusy
desc=#PostTessellation#The percentage of GPUTime scalar ALU instructions are being processed for the DS and GS in a pipeline that uses tessellation.
type=gpa_float64
usage=percentage
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
SQ_GS*_PERF_SEL_INSTS_SALU[0..3]
SPI*_PERF_HS_WAVE[0..3]
CPF_PERF_SEL_CPF_STAT_BUSY
eqn=(0),0..3,sum4,NUM_CUS,/,8,/,(100),*,4..7,sum4,ifnotzero

name=PostTessSALUBusyCycles
desc=#PostTessellation#Number of GPU cycles where scalar ALU instructions are being processed for the DS and GS in a pipeline that uses tessellation.
type=gpa_float64
usage=cycles
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
SQ_GS*_PERF_SEL_INSTS_SALU[0..3]
SPI*_PERF_HS_WAVE[0..3]
eqn=(0),0..3,sum4,NUM_CUS,/,4..7,sum4,ifnotzero

name=PrimitiveAssemblyBusy
desc=#Timing#The percentage of GPUTime that primitive assembly (clipping and culling) is busy. High values may be caused by having many small primitives; mid to low values may indicate pixel shader or output buffer bottleneck.
type=gpa_float64
usage=percentage
spm=0
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
PA_SU*_PERF_PAPC_CLIP_BUSY[0..3]
PA_SU*_PERF_PAPC_SU_STALLED_SC[0..3]
PA_SU*_PERF_PAPC_SU_OUTPUT_PRIM[0..3]
PA_SU*_PERF_PAPC_SU_OUTPUT_PRIM_DUAL[0..3]
PA_SU*_PERF_PAPC_SU_OUTPUT_CLIP_PRIM[0..3]
PA_SU*_PERF_PAPC_SU_OUTPUT_CLIP_PRIM_DUAL[0..3]
CPF_PERF_SEL_CPF_STAT_BUSY
eqn=0,4,-,8,12,+,16,+,20,(2),*,+,SU_CLOCKS_PRIM,*,-,1,5,-,9,13,+,17,+,21,(2),*,+,SU_CLOCKS_PRIM,*,-,max,2,6,-,10,14,+,18,+,22,(2),*,+,SU_CLOCKS_PRIM,*,-,max,3,7,-,11,15,+,19,+,23,(2),*,+,SU_CLOCKS_PRIM,*,-,max,(0),max,24,/,(100),*,(100),min

name=PrimitiveAssemblyBusyCycles
desc=#Timing#Number of GPU cycles the primitive assembly (clipping and culling) is busy. High values may be caused by having many small primitives; mid to low values may indicate pixel shader or output buffer bottleneck.
type=gpa_float64
usage=cycles
spm=0
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
PA_SU*_PERF_PAPC_CLIP_BUSY[0..3]
PA_SU*_PERF_PAPC_SU_STALLED_SC[0..3]
PA_SU*_PERF_PAPC_SU_OUTPUT_PRIM[0..3]
PA_SU*_PERF_PAPC_SU_OUTPUT_PRIM_DUAL[0..3]
PA_SU*_PERF_PAPC_SU_OUTPUT_CLIP_PRIM[0..3]
PA_SU*_PERF_PAPC_SU_OUTPUT_CLIP_PRIM_DUAL[0..3]
eqn=0,4,-,8,12,+,16,+,20,(2),*,+,SU_CLOCKS_PRIM,*,-,1,5,-,9,13,+,17,+,21,(2),*,+,SU_CLOCKS_PRIM,*,-,max,2,6,-,10,14,+,18,+,22,(2),*,+,SU_CLOCKS_PRIM,*,-,max,3,7,-,11,15,+,19,+,23,(2),*,+,SU_CLOCKS_PRIM,*,-,max,(0),max

name=PrimitivesIn
desc=#PrimitiveAssembly#The number of primitives received by the hardware. This includes primitives generated by tessellation.
type=gpa_float64
usage=items
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
PA_SU*_PERF_PAPC_PA_INPUT_PRIM[0..3]
eqn=0,1,2,3,sum4

name=CulledPrims
desc=#PrimitiveAssembly#The number of culled primitives. Typical reasons include scissor, the primitive having zero area, and back or front face culling.
type=gpa_float64
usage=items
spm=0
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
PA_SU*_PERF_PAPC_CLPR_CULL_PRIM[0..3]
PA_SU*_PERF_PAPC_SU_ZERO_AREA_CULL_PRIM[0..3]
PA_SU*_PERF_PAPC_SU_BACK_FACE_CULL_PRIM[0..3]
PA_SU*_PERF_PAPC_SU_FRONT_FACE_CULL_PRIM[0..3]
PA_SU*_PERF_PAPC_SU_POLYMODE_FACE_CULL[0..3]
eqn=0..19,sum20

name=ClippedPrims
desc=#PrimitiveAssembly#The number of primitives that required one or more clipping operations due to intersecting the view volume or user clip planes.
type=gpa_float64
usage=items
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
PA_SU*_PERF_PAPC_CLPR_VVUCP_CLIP_PRIM[0..3]
eqn=0..3,sum4

name=PAStalledOnRasterizer
desc=#PrimitiveAssembly#Percentage of GPUTime that primitive assembly waits for rasterization to be ready to accept data. This roughly indicates for what percentage of time the pipeline is bottlenecked by pixel operations.
type=gpa_float64
usage=percentage
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
PA_SU*_PERF_PAPC_SU_STALLED_SC[0..3]
CPF_PERF_SEL_CPF_STAT_BUSY
eqn=0..3,max4,4,/,(100),*

name=PAStalledOnRasterizerCycles
desc=#PrimitiveAssembly#Number of GPU cycles the primitive assembly waits for rasterization to be ready to accept data. Indicates the number of GPU cycles the pipeline is bottlenecked by pixel operations.
type=gpa_float64
usage=cycles
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
PA_SU*_PERF_PAPC_SU_STALLED_SC[0..3]
eqn=0..3,max4

name=PSPixelsOut
desc=#PixelShader#Pixels exported from shader to color buffers. Does not include killed or alpha tested pixels; if there are multiple render targets, each render target receives one export, so this will be 2 for 1 pixel written to two RTs.
type=gpa_float64
usage=items
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
SX*_PERF_SEL_DB0_PIXELS[0..7]
SX*_PERF_SEL_DB1_PIXELS[0..7]
SX*_PERF_SEL_DB2_PIXELS[0..7]
SX*_PERF_SEL_DB3_PIXELS[0..7]
eqn=0..31,sum32

name=PSExportStalls
desc=#PixelShader#Pixel shader output stalls. Percentage of GPUBusy. Should be zero for PS or further upstream limited cases; if not zero, indicates a bottleneck in late Z testing or in the color buffer.
type=gpa_float64
usage=percentage
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
SX*_PERF_SEL_DB0_PIXEL_STALL[0..7]
SX*_PERF_SEL_DB1_PIXEL_STALL[0..7]
SX*_PERF_SEL_DB2_PIXEL_STALL[0..7]
SX*_PERF_SEL_DB3_PIXEL_STALL[0..7]
CPF_PERF_SEL_CPF_STAT_BUSY
eqn=0..31,max32,32,/,(100),*

name=PSExportStallsCycles
desc=#PixelShader#Number of GPU cycles the pixel shader output stalls. Should be zero for PS or further upstream limited cases; if not zero, indicates a bottleneck in late Z testing or in the color buffer.
type=gpa_float64
usage=cycles
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
SX*_PERF_SEL_DB0_PIXEL_STALL[0..7]
SX*_PERF_SEL_DB1_PIXEL_STALL[0..7]
SX*_PERF_SEL_DB2_PIXEL_STALL[0..7]
SX*_PERF_SEL_DB3_PIXEL_STALL[0..7]
eqn=0..31,max32

name=CSThreadGroupsLaunched
desc=#ComputeShader#Total number of thread groups launched.
type=gpa_float64
usage=items
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
SPI*_PERF_CSGN_NUM_THREADGROUPS[0..3]
SPI*_PERF_CSN_NUM_THREADGROUPS[0..3]
eqn=0..7,sum8

name=CSWavefrontsLaunched
desc=#ComputeShader#The total number of wavefronts launched for the CS.
type=gpa_float64
usage=items
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
SPI*_PERF_CSGN_WAVE[0..3]
SPI*_PERF_CSN_WAVE[0..3]
eqn=0..7,sum8

name=CSThreadsLaunched
desc=#ComputeShader#The number of CS threads launched and processed by the hardware.
type=gpa_float64
usage=items
[DX11Gfx103]
SQ_CS*_PERF_SEL_ITEMS[0..3]
SPI*_PERF_CSGN_NUM_THREADGROUPS[0..3]
SPI*_PERF_CSN_NUM_THREADGROUPS[0..3]
eqn=(0),SQ_CS*_PERF_SEL_ITEMS,sum4,SPI*_PERF_CSGN_NUM_THREADGROUPS,SPI*_PERF_CSN_NUM_THREADGROUPS,sum8,ifnotzero
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
SQ_CS*_PERF_SEL_ITEMS[0..3]
eqn=SQ_CS*_PERF_SEL_ITEMS,sum4

name=CSThreadGroupSize
desc=#ComputeShader#The number of CS threads within each thread group.
type=gpa_float64
usage=items
spm=0
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
SQ_CS*_PERF_SEL_ITEMS[0..3]
SPI*_PERF_CSGN_NUM_THREADGROUPS[0..3]
SPI*_PERF_CSN_NUM_THREADGROUPS[0..3]
eqn=(0),SQ_CS*_PERF_SEL_ITEMS,sum4,SPI*_PERF_CSGN_NUM_THREADGROUPS,SPI*_PERF_CSN_NUM_THREADGROUPS,sum8,/,SPI*_PERF_CSGN_NUM_THREADGROUPS,SPI*_PERF_CSN_NUM_THREADGROUPS,sum8,ifnotzero

name=CSVALUInsts
desc=#ComputeShader#The average number of vector ALU instructions executed per work-item (affected by flow control).
type=gpa_float64
usage=items
spm=0
[DX11Gfx103]
SQ_CS*_PERF_SEL_INSTS_VALU[0..3]
SQ_CS*_PERF_SEL_WAVES[0..3]
SPI*_PERF_CSGN_NUM_THREADGROUPS[0..3]
SPI*_PERF_CSN_NUM_THREADGROUPS[0..3]
eqn=(0),SQ_CS*_PERF_SEL_INSTS_VALU,sum4,SQ_CS*_PERF_SEL_WAVES,sum4,/,SPI*_PERF_CSGN_NUM_THREADGROUPS,SPI*_PERF_CSN_NUM_THREADGROUPS,sum8,ifnotzero
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
SQ_CS*_PERF_SEL_INSTS_VALU[0..3]
SQ_CS*_PERF_SEL_WAVES[0..3]
eqn=SQ_CS*_PERF_SEL_INSTS_VALU,sum4,SQ_CS*_PERF_SEL_WAVES,sum4,/

name=CSVALUUtilization
desc=#ComputeShader#The percentage of active vector ALU threads in a wave. A lower number can mean either more thread divergence in a wave or that the work-group size is not a multiple of the wave size. Value range: 0% (bad), 100% (ideal - no thread divergence).
type=gpa_float64
usage=percentage
spm=0
[DX11Gfx103]
SQ_CS*_PERF_SEL_ITEM_CYCLES_VALU[0..3]
SQ_CS*_PERF_SEL_INST_CYCLES_VALU[0..3]
SQ_CS*_PERF_SEL_WAVES_32[0..3]
SPI*_PERF_CSGN_NUM_THREADGROUPS[0..3]
SPI*_PERF_CSN_NUM_THREADGROUPS[0..3]
eqn=(0),(0),SQ_CS*_PERF_SEL_ITEM_CYCLES_VALU,sum4,SQ_CS*_PERF_SEL_INST_CYCLES_VALU,sum4,(64),(32),SQ_CS*_PERF_SEL_WAVES_32,sum4,ifnotzero,*,/,(100),*,SQ_CS*_PERF_SEL_INST_CYCLES_VALU,sum4,ifnotzero,(100),min,SPI*_PERF_CSGN_NUM_THREADGROUPS,SPI*_PERF_CSN_NUM_THREADGROUPS,sum8,ifnotzero
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
SQ_CS*_PERF_SEL_ITEM_CYCLES_VALU[0..3]
SQ_CS*_PERF_SEL_INST_CYCLES_VALU[0..3]
SQ_CS*_PERF_SEL_WAVES_32[0..3]
eqn=(0),SQ_CS*_PERF_SEL_ITEM_CYCLES_VALU,sum4,SQ_CS*_PERF_SEL_INST_CYCLES_VALU,sum4,(64),(32),SQ_CS*_PERF_SEL_WAVES_32,sum4,ifnotzero,*,/,(100),*,SQ_CS*_PERF_SEL_INST_CYCLES_VALU,sum4,ifnotzero,(100),min

name=CSSALUInsts
desc=#ComputeShader#The average number of scalar ALU instructions executed per work-item (affected by flow control).
type=gpa_float64
usage=items
spm=0
[DX11Gfx103]
SQ_CS*_PERF_SEL_INSTS_SALU[0..3]
SQ_CS*_PERF_SEL_WAVES[0..3]
SPI*_PERF_CSGN_NUM_THREADGROUPS[0..3]
SPI*_PERF_CSN_NUM_THREADGROUPS[0..3]
eqn=(0),(0),SQ_CS*_PERF_SEL_INSTS_SALU,sum4,SQ_CS*_PERF_SEL_WAVES,sum4,/,SQ_CS*_PERF_SEL_WAVES,sum4,ifnotzero,SPI*_PERF_CSGN_NUM_THREADGROUPS,SPI*_PERF_CSN_NUM_THREADGROUPS,sum8,ifnotzero
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
SQ_CS*_PERF_SEL_INSTS_SALU[0..3]
SQ_CS*_PERF_SEL_WAVES[0..3]
eqn=(0),SQ_CS*_PERF_SEL_INSTS_SALU,sum4,SQ_CS*_PERF_SEL_WAVES,sum4,/,SQ_CS*_PERF_SEL_WAVES,sum4,ifnotzero

name=CSVFetchInsts
desc=#ComputeShader#The average number of vector fetch instructions from the video memory executed per work-item (affected by flow control).
type=gpa_float64
usage=items
spm=0
[DX11Gfx103]
SQ_CS*_PERF_SEL_INSTS_TEX_LOAD[0..3]
SQ_CS*_PERF_SEL_WAVES[0..3]
SPI*_PERF_CSGN_NUM_THREADGROUPS[0..3]
SPI*_PERF_CSN_NUM_THREADGROUPS[0..3]
eqn=(0),(0),SQ_CS*_PERF_SEL_INSTS_TEX_LOAD,sum4,SQ_CS*_PERF_SEL_WAVES,sum4,/,SQ_CS*_PERF_SEL_WAVES,sum4,ifnotzero,SPI*_PERF_CSGN_NUM_THREADGROUPS,SPI*_PERF_CSN_NUM_THREADGROUPS,sum8,ifnotzero
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
SQ_CS*_PERF_SEL_INSTS_TEX_LOAD[0..3]
SQ_CS*_PERF_SEL_WAVES[0..3]
eqn=(0),SQ_CS*_PERF_SEL_INSTS_TEX_LOAD,sum4,SQ_CS*_PERF_SEL_WAVES,sum4,/,SQ_CS*_PERF_SEL_WAVES,sum4,ifnotzero

name=CSSFetchInsts
desc=#ComputeShader#The average number of scalar fetch instructions from the video memory executed per work-item (affected by flow control).
type=gpa_float64
usage=items
spm=0
[DX11Gfx103]
SQ_CS*_PERF_SEL_INSTS_SMEM[0..3]
SQ_CS*_PERF_SEL_WAVES[0..3]
SPI*_PERF_CSGN_NUM_THREADGROUPS[0..3]
SPI*_PERF_CSN_NUM_THREADGROUPS[0..3]
eqn=(0),(0),SQ_CS*_PERF_SEL_INSTS_SMEM,sum4,SQ_CS*_PERF_SEL_WAVES,sum4,/,SQ_CS*_PERF_SEL_WAVES,sum4,ifnotzero,SPI*_PERF_CSGN_NUM_THREADGROUPS,SPI*_PERF_CSN_NUM_THREADGROUPS,sum8,ifnotzero
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
SQ_CS*_PERF_SEL_INSTS_SMEM[0..3]
SQ_CS*_PERF_SEL_WAVES[0..3]
eqn=(0),SQ_CS*_PERF_SEL_INSTS_SMEM,sum4,SQ_CS*_PERF_SEL_WAVES,sum4,/,SQ_CS*_PERF_SEL_WAVES,sum4,ifnotzero

name=CSVWriteInsts
desc=#ComputeShader#The average number of vector write instructions to the video memory executed per work-item (affected by flow control).
type=gpa_float64
usage=items
spm=0
[DX11Gfx103]
SQ_CS*_PERF_SEL_INSTS_TEX_STORE[0..3]
SQ_CS*_PERF_SEL_WAVES[0..3]
SPI*_PERF_CSGN_NUM_THREADGROUPS[0..3]
SPI*_PERF_CSN_NUM_THREADGROUPS[0..3]
eqn=(0),(0),SQ_CS*_PERF_SEL_INSTS_TEX_STORE,sum4,SQ_CS*_PERF_SEL_WAVES,sum4,/,SQ_CS*_PERF_SEL_WAVES,sum4,ifnotzero,SPI*_PERF_CSGN_NUM_THREADGROUPS,SPI*_PERF_CSN_NUM_THREADGROUPS,sum8,ifnotzero
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
SQ_CS*_PERF_SEL_INSTS_TEX_STORE[0..3]
SQ_CS*_PERF_SEL_WAVES[0..3]
eqn=(0),SQ_CS*_PERF_SEL_INSTS_TEX_STORE,sum4,SQ_CS*_PERF_SEL_WAVES,sum4,/,SQ_CS*_PERF_SEL_WAVES,sum4,ifnotzero

name=CSGDSInsts
desc=#ComputeShader#The average number of GDS read or GDS write instructions executed per work item (affected by flow control).
type=gpa_float64
usage=items
spm=0
[DX11Gfx103]
SQ_CS*_PERF_SEL_INSTS_GDS[0..3]
SQ_CS*_PERF_SEL_WAVES[0..3]
SPI*_PERF_CSGN_NUM_THREADGROUPS[0..3]
SPI*_PERF_CSN_NUM_THREADGROUPS[0..3]
eqn=(0),(0),SQ_CS*_PERF_SEL_INSTS_GDS,sum4,SQ_CS*_PERF_SEL_WAVES,sum4,/,SQ_CS*_PERF_SEL_WAVES,sum4,ifnotzero,SPI*_PERF_CSGN_NUM_THREADGROUPS,SPI*_PERF_CSN_NUM_THREADGROUPS,sum8,ifnotzero
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
SQ_CS*_PERF_SEL_INSTS_GDS[0..3]
SQ_CS*_PERF_SEL_WAVES[0..3]
eqn=(0),SQ_CS*_PERF_SEL_INSTS_GDS,sum4,SQ_CS*_PERF_SEL_WAVES,sum4,/,SQ_CS*_PERF_SEL_WAVES,sum4,ifnotzero

name=CSLDSInsts
desc=#ComputeShader#The average number of LDS read/write instructions executed per work-item (affected by flow control).
type=gpa_float64
usage=items
spm=0
[DX11Gfx103]
SQ_CS*_PERF_SEL_INSTS_LDS[0..3]
SQ_CS*_PERF_SEL_WAVES[0..3]
SPI*_PERF_CSGN_NUM_THREADGROUPS[0..3]
SPI*_PERF_CSN_NUM_THREADGROUPS[0..3]
eqn=(0),(0),SQ_CS*_PERF_SEL_INSTS_LDS,sum4,SQ_CS*_PERF_SEL_WAVES,sum4,/,SQ_CS*_PERF_SEL_WAVES,sum4,ifnotzero,SPI*_PERF_CSGN_NUM_THREADGROUPS,SPI*_PERF_CSN_NUM_THREADGROUPS,sum8,ifnotzero
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
SQ_CS*_PERF_SEL_INSTS_LDS[0..3]
SQ_CS*_PERF_SEL_WAVES[0..3]
eqn=(0),SQ_CS*_PERF_SEL_INSTS_LDS,sum4,SQ_CS*_PERF_SEL_WAVES,sum4,/,SQ_CS*_PERF_SEL_WAVES,sum4,ifnotzero

name=CSALUStalledByLDS
desc=#ComputeShader#The percentage of GPUTime ALU units are stalled by the LDS input queue being full or the output queue being not ready. If there are LDS bank conflicts, reduce them. Otherwise, try reducing the number of LDS accesses if possible. Value range: 0% (optimal) to 100% (bad).
type=gpa_float64
usage=percentage
spm=0
[DX11Gfx103]
SQ_CS*_PERF_SEL_WAIT_INST_LDS[0..3]
SQ_CS*_PERF_SEL_WAVES[0..3]
CPF_PERF_SEL_CPF_STAT_BUSY
SPI*_PERF_CSGN_NUM_THREADGROUPS[0..3]
SPI*_PERF_CSN_NUM_THREADGROUPS[0..3]
eqn=(0),SQ_CS*_PERF_SEL_WAIT_INST_LDS,sum4,SQ_CS*_PERF_SEL_WAVES,sum4,/,CPF_PERF_SEL_CPF_STAT_BUSY,/,(100),*,SPI*_PERF_CSGN_NUM_THREADGROUPS,SPI*_PERF_CSN_NUM_THREADGROUPS,sum8,ifnotzero
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
SQ_CS*_PERF_SEL_WAIT_INST_LDS[0..3]
SQ_CS*_PERF_SEL_WAVES[0..3]
CPF_PERF_SEL_CPF_STAT_BUSY
eqn=SQ_CS*_PERF_SEL_WAIT_INST_LDS,sum4,SQ_CS*_PERF_SEL_WAVES,sum4,/,CPF_PERF_SEL_CPF_STAT_BUSY,/,(100),*

name=CSALUStalledByLDSPerWave
desc=#ComputeShader#The average percentage of GPUTime each wavefront's ALU units are stalled by the LDS input queue being full or the output queue being not ready. If there are LDS bank conflicts, reduce them. Otherwise, try reducing the number of LDS accesses if possible. Value range: 0% (optimal) to 100% (bad).
type=gpa_float64
usage=percentage
spm=1
discrete=0
[DX12Gfx103]
SQ_CS*_PERF_SEL_WAIT_INST_LDS[0..3]
SQ_CS*_PERF_SEL_LEVEL_WAVES[0..3]
CPF_PERF_SEL_CPF_STAT_BUSY
eqn=SQ_CS*_PERF_SEL_WAIT_INST_LDS,sum4,SQ_CS*_PERF_SEL_LEVEL_WAVES,sum4,/,CPF_PERF_SEL_CPF_STAT_BUSY,/,(100),*

name=CSALUStalledByLDSCycles
desc=#ComputeShader#The average number of GPU cycles the each wavefronts' ALU units are stalled by the LDS input queue being full or the output queue being not ready. If there are LDS bank conflicts, reduce them. Otherwise, try reducing the number of LDS accesses if possible.
type=gpa_float64
usage=cycles
spm=0
[DX11Gfx103]
SQ_CS*_PERF_SEL_WAIT_INST_LDS[0..3]
SQ_CS*_PERF_SEL_WAVES[0..3]
SPI*_PERF_CSGN_NUM_THREADGROUPS[0..3]
SPI*_PERF_CSN_NUM_THREADGROUPS[0..3]
eqn=(0),SQ_CS*_PERF_SEL_WAIT_INST_LDS,sum4,SQ_CS*_PERF_SEL_WAVES,sum4,/,SPI*_PERF_CSGN_NUM_THREADGROUPS,SPI*_PERF_CSN_NUM_THREADGROUPS,sum8,ifnotzero
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
SQ_CS*_PERF_SEL_WAIT_INST_LDS[0..3]
SQ_CS*_PERF_SEL_WAVES[0..3]
eqn=SQ_CS*_PERF_SEL_WAIT_INST_LDS,sum4,SQ_CS*_PERF_SEL_WAVES,sum4,/

name=CSLDSBankConflict
desc=#ComputeShader#The percentage of GPUTime LDS is stalled by bank conflicts. Value range: 0% (optimal) to 100% (bad).
type=gpa_float64
usage=percentage
[DX11Gfx103]
SQ_CS*_SQC_PERF_SEL_LDS_BANK_CONFLICT[0..3]
CPF_PERF_SEL_CPF_STAT_BUSY
SPI*_PERF_CSGN_NUM_THREADGROUPS[0..3]
SPI*_PERF_CSN_NUM_THREADGROUPS[0..3]
eqn=(0),SQ_CS*_SQC_PERF_SEL_LDS_BANK_CONFLICT,sum4,NUM_SIMDS,/,CPF_PERF_SEL_CPF_STAT_BUSY,/,(100),*,SPI*_PERF_CSGN_NUM_THREADGROUPS,SPI*_PERF_CSN_NUM_THREADGROUPS,sum8,ifnotzero
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
SQ_CS*_SQC_PERF_SEL_LDS_BANK_CONFLICT[0..3]
CPF_PERF_SEL_CPF_STAT_BUSY
eqn=SQ_CS*_SQC_PERF_SEL_LDS_BANK_CONFLICT,sum4,NUM_SIMDS,/,CPF_PERF_SEL_CPF_STAT_BUSY,/,(100),*

name=CSLDSBankConflictCycles
desc=#ComputeShader#Number of GPU cycles the LDS is stalled by bank conflicts. Value range: 0 (optimal) to GPUBusyCycles (bad).
type=gpa_float64
usage=cycles
[DX11Gfx103]
SQ_CS*_SQC_PERF_SEL_LDS_BANK_CONFLICT[0..3]
SPI*_PERF_CSGN_NUM_THREADGROUPS[0..3]
SPI*_PERF_CSN_NUM_THREADGROUPS[0..3]
eqn=(0),0..3,sum4,NUM_SIMDS,/,SPI*_PERF_CSGN_NUM_THREADGROUPS,SPI*_PERF_CSN_NUM_THREADGROUPS,sum8,ifnotzero
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
SQ_CS*_SQC_PERF_SEL_LDS_BANK_CONFLICT[0..3]
eqn=0..3,sum4,NUM_SIMDS,/

name=TexUnitBusy
desc=#Timing#The percentage of GPUTime the texture unit is active. This is measured with all extra fetches and any cache or memory effects taken into account.
type=gpa_float64
usage=percentage
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
TA*_PERF_SEL_TA_BUSY[0..79]
CPF_PERF_SEL_CPF_STAT_BUSY
eqn=0..79,max80,80,/,(100),*

name=TexUnitBusyCycles
desc=#Timing#Number of GPU cycles the texture unit is active. This is measured with all extra fetches and any cache or memory effects taken into account.
type=gpa_float64
usage=cycles
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
TA*_PERF_SEL_TA_BUSY[0..79]
eqn=0..79,max80

name=TexTriFilteringPct
desc=#TextureUnit#Percentage of pixels that received trilinear filtering. Note that not all pixels for which trilinear filtering is enabled will receive it (e.g. if the texture is magnified).
type=gpa_float64
usage=percentage
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
TA*_PERF_SEL_MIP_2_CYCLE_QUADS[0..63]
TA*_PERF_SEL_MIP_1_CYCLE_QUADS[0..63]
eqn=0..63,sum64,64..127,sum64,0..63,sum64,+,/,(100),*

name=TexTriFilteringCount
desc=#TextureUnit#Count of pixels that received trilinear filtering. Note that not all pixels for which trilinear filtering is enabled will receive it (e.g. if the texture is magnified).
type=gpa_float64
usage=items
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
TA*_PERF_SEL_MIP_2_CYCLE_QUADS[0..63]
eqn=0..63,sum64,(4),*

name=NoTexTriFilteringCount
desc=#TextureUnit#Count of pixels that did not receive trilinear filtering.
type=gpa_float64
usage=items
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
TA*_PERF_SEL_MIP_1_CYCLE_QUADS[0..63]
eqn=0..63,sum64,(4),*

name=TexVolFilteringPct
desc=#TextureUnit#Percentage of pixels that received volume filtering.
type=gpa_float64
usage=percentage
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
TA*_PERF_SEL_VOL_2_CYCLE_QUADS[0..63]
TA*_PERF_SEL_VOL_1_CYCLE_QUADS[0..63]
eqn=0..63,sum64,64..127,sum64,0..63,sum64,+,/,(100),*

name=TexVolFilteringCount
desc=#TextureUnit#Count of pixels that received volume filtering.
type=gpa_float64
usage=items
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
TA*_PERF_SEL_VOL_2_CYCLE_QUADS[0..63]
eqn=0..63,sum64,(4),*

name=NoTexVolFilteringCount
desc=#TextureUnit#Count of pixels that did not receive volume filtering.
type=gpa_float64
usage=items
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
TA*_PERF_SEL_VOL_1_CYCLE_QUADS[0..63]
eqn=0..63,sum64,(4),*

name=TexAveAnisotropy
desc=#TextureUnit#The average degree of anisotropy applied. A number between 1 and 16. The anisotropic filtering algorithm only applies samples where they are required (e.g. there will be no extra anisotropic samples if the view vector is perpendicular to the surface) so this can be much lower than the requested anisotropy.
type=gpa_float64
usage=Items
spm=0
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
TA*_PERF_SEL_ANISO_1_CYCLE_QUADS[0..79]
TA*_PERF_SEL_ANISO_2_CYCLE_QUADS[0..79]
TA*_PERF_SEL_ANISO_4_CYCLE_QUADS[0..79]
TA*_PERF_SEL_ANISO_6_CYCLE_QUADS[0..79]
TA*_PERF_SEL_ANISO_8_CYCLE_QUADS[0..79]
TA*_PERF_SEL_ANISO_10_CYCLE_QUADS[0..79]
TA*_PERF_SEL_ANISO_12_CYCLE_QUADS[0..79]
TA*_PERF_SEL_ANISO_14_CYCLE_QUADS[0..79]
TA*_PERF_SEL_ANISO_16_CYCLE_QUADS[0..79]
eqn=0..79,sum80,(2),80..159,sum80,*,+,(4),160..239,sum80,*,+,(6),240..319,sum80,*,+,(8),320..399,sum80,*,+,(10),400..479,sum80,*,+,(12),480..559,sum80,*,+,(14),560..639,sum80,*,+,(16),640..719,sum80,*,+,0..79,sum80,80..159,sum80,+,160..239,sum80,+,240..319,sum80,+,320..399,sum80,+,400..479,sum80,+,480..559,sum80,+,560..639,sum80,+,640..719,sum80,+,/

name=DepthStencilTestBusy
desc=#Timing#Percentage of time GPU spent performing depth and stencil tests relative to GPUBusy.
type=gpa_float64
usage=percentage
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
DB*_PERF_SEL_OP_PIPE_BUSY[0..15]
CPF_PERF_SEL_CPF_STAT_BUSY
eqn=0..15,max16,16,/,(100),*

name=DepthStencilTestBusyCycles
desc=#Timing#Number of GPU cycles spent performing depth and stencil tests.
type=gpa_float64
usage=cycles
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
DB*_PERF_SEL_OP_PIPE_BUSY[0..15]
eqn=0..15,max16

name=HiZTilesAccepted
desc=#DepthAndStencil#Percentage of tiles accepted by HiZ and will be rendered to the depth or color buffers.
type=gpa_float64
usage=percentage
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
DB*_PERF_SEL_DB_SC_TILE_CULLED[0..15]
DB*_PERF_SEL_SC_DB_TILE_TILES[0..15]
eqn=16..31,sum16,0..15,sum16,-,(0),max,16..31,sum16,/,(100),*

name=HiZTilesAcceptedCount
desc=#DepthAndStencil#Count of tiles accepted by HiZ and will be rendered to the depth or color buffers.
type=gpa_float64
usage=items
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
DB*_PERF_SEL_SC_DB_TILE_TILES[0..15]
DB*_PERF_SEL_DB_SC_TILE_CULLED[0..15]
eqn=0..15,sum16,16..31,sum16,-,(0),max

name=HiZTilesRejectedCount
desc=#DepthAndStencil#Count of tiles not accepted by HiZ.
type=gpa_float64
usage=items
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
DB*_PERF_SEL_DB_SC_TILE_CULLED[0..15]
eqn=0..15,sum16

name=PreZTilesDetailCulled
desc=#DepthAndStencil#Percentage of tiles rejected because the associated prim had no contributing area.
type=gpa_float64
usage=percentage
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
DB*_PERF_SEL_SC_DB_TILE_TILES[0..15]
DB*_PERF_SEL_SC_DB_QUAD_KILLED_TILES[0..15]
eqn=16..31,sum16,0..15,sum16,/,(100),*

name=PreZTilesDetailCulledCount
desc=#DepthAndStencil#Count of tiles rejected because the associated primitive had no contributing area.
type=gpa_float64
usage=items
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
[DX11Gfx103]
DB*_PERF_SEL_SC_DB_QUAD_KILLED_TILES[0..15]
eqn=0..15,sum16

name=PreZTilesDetailSurvivingCount
desc=#DepthAndStencil#Count of tiles surviving because the associated primitive had contributing area.
type=gpa_float64
usage=items
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
[DX11Gfx103]
DB*_PERF_SEL_SC_DB_TILE_TILES[0..15]
eqn=0..15,sum16

name=HiZQuadsCulled
desc=#DepthAndStencil#Percentage of quads that did not have to continue on in the pipeline after HiZ. They may be written directly to the depth buffer, or culled completely. Consistently low values here may suggest that the Z-range is not being fully utilized.
type=gpa_float64
usage=percentage
spm=0
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
PA_SC*_QZ0_QUAD_COUNT[0..15]
PA_SC*_QZ1_QUAD_COUNT[0..15]
PA_SC*_QZ2_QUAD_COUNT[0..15]
PA_SC*_QZ3_QUAD_COUNT[0..15]
PA_SC*_P0_HIZ_QUAD_COUNT[0..15]
PA_SC*_P1_HIZ_QUAD_COUNT[0..15]
PA_SC*_P2_HIZ_QUAD_COUNT[0..15]
PA_SC*_P3_HIZ_QUAD_COUNT[0..15]
eqn=0..63,sum64,64..127,sum64,-,(0),max,0..63,sum64,/,(100),*

name=HiZQuadsCulledCount
desc=#DepthAndStencil#Count of quads that did not have to continue on in the pipeline after HiZ. They may be written directly to the depth buffer, or culled completely. Consistently low values here may suggest that the Z-range is not being fully utilized.
type=gpa_float64
usage=items
spm=0
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
PA_SC*_QZ0_QUAD_COUNT[0..15]
PA_SC*_QZ1_QUAD_COUNT[0..15]
PA_SC*_QZ2_QUAD_COUNT[0..15]
PA_SC*_QZ3_QUAD_COUNT[0..15]
PA_SC*_P0_HIZ_QUAD_COUNT[0..15]
PA_SC*_P1_HIZ_QUAD_COUNT[0..15]
PA_SC*_P2_HIZ_QUAD_COUNT[0..15]
PA_SC*_P3_HIZ_QUAD_COUNT[0..15]
eqn=0..63,sum64,64..127,sum64,-,(0),max

name=HiZQuadsAcceptedCount
desc=#DepthAndStencil#Count of quads that did continue on in the pipeline after HiZ.
type=gpa_float64
usage=items
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
PA_SC*_P0_HIZ_QUAD_COUNT[0..15]
PA_SC*_P1_HIZ_QUAD_COUNT[0..15]
PA_SC*_P2_HIZ_QUAD_COUNT[0..15]
PA_SC*_P3_HIZ_QUAD_COUNT[0..15]
eqn=0..63,sum64

name=PreZQuadsCulled
desc=#DepthAndStencil#Percentage of quads rejected based on the detailZ and earlyZ tests.
type=gpa_float64
usage=percentage
spm=0
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
PA_SC*_P0_HIZ_QUAD_COUNT[0..15]
PA_SC*_P1_HIZ_QUAD_COUNT[0..15]
PA_SC*_P2_HIZ_QUAD_COUNT[0..15]
PA_SC*_P3_HIZ_QUAD_COUNT[0..15]
PA_SC*_EARLYZ_QUAD_COUNT[0..15]
PA_SC*_QZ0_QUAD_COUNT[0..15]
PA_SC*_QZ1_QUAD_COUNT[0..15]
PA_SC*_QZ2_QUAD_COUNT[0..15]
PA_SC*_QZ3_QUAD_COUNT[0..15]
eqn=0..63,sum64,64..79,sum16,-,(0),max,80..143,sum64,/,(100),*

name=PreZQuadsCulledCount
desc=#DepthAndStencil#Count of quads rejected based on the detailZ and earlyZ tests.
type=gpa_float64
usage=items
spm=0
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
PA_SC*_P0_HIZ_QUAD_COUNT[0..15]
PA_SC*_P1_HIZ_QUAD_COUNT[0..15]
PA_SC*_P2_HIZ_QUAD_COUNT[0..15]
PA_SC*_P3_HIZ_QUAD_COUNT[0..15]
PA_SC*_EARLYZ_QUAD_COUNT[0..15]
eqn=0..63,sum64,64..79,sum16,-,(0),max

name=PreZQuadsSurvivingCount
desc=#DepthAndStencil#Count of quads surviving detailZ and earlyZ tests.
type=gpa_float64
usage=items
spm=0
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
PA_SC*_EARLYZ_QUAD_COUNT[0..15]
eqn=0..15,sum16

name=PostZQuads
desc=#DepthAndStencil#Percentage of quads for which the pixel shader will run and may be postZ tested.
type=gpa_float64
usage=percentage
spm=0
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
PA_SC*_EARLYZ_QUAD_COUNT[0..15]
PA_SC*_QZ0_QUAD_COUNT[0..15]
PA_SC*_QZ1_QUAD_COUNT[0..15]
PA_SC*_QZ2_QUAD_COUNT[0..15]
PA_SC*_QZ3_QUAD_COUNT[0..15]
eqn=0..15,sum16,16..79,sum64,/,(100),*

name=PostZQuadCount
desc=#DepthAndStencil#Count of quads for which the pixel shader will run and may be postZ tested.
type=gpa_float64
usage=items
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
PA_SC*_EARLYZ_QUAD_COUNT[0..15]
eqn=0..15,sum16

name=PreZSamplesPassing
desc=#DepthAndStencil#Number of samples tested for Z before shading and passed.
type=gpa_float64
usage=items
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
DB*_PERF_SEL_PREZ_SAMPLES_PASSING_Z[0..15]
eqn=0..15,sum16

name=PreZSamplesFailingS
desc=#DepthAndStencil#Number of samples tested for Z before shading and failed stencil test.
type=gpa_float64
usage=items
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
DB*_PERF_SEL_PREZ_SAMPLES_FAILING_S[0..15]
eqn=0..15,sum16

name=PreZSamplesFailingZ
desc=#DepthAndStencil#Number of samples tested for Z before shading and failed Z test.
type=gpa_float64
usage=items
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
DB*_PERF_SEL_PREZ_SAMPLES_FAILING_Z[0..15]
eqn=0..15,sum16

name=PostZSamplesPassing
desc=#DepthAndStencil#Number of samples tested for Z after shading and passed.
type=gpa_float64
usage=items
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
DB*_PERF_SEL_POSTZ_SAMPLES_PASSING_Z[0..15]
eqn=0..15,sum16

name=PostZSamplesFailingS
desc=#DepthAndStencil#Number of samples tested for Z after shading and failed stencil test.
type=gpa_float64
usage=items
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
DB*_PERF_SEL_POSTZ_SAMPLES_FAILING_S[0..15]
eqn=0..15,sum16

name=PostZSamplesFailingZ
desc=#DepthAndStencil#Number of samples tested for Z after shading and failed Z test.
type=gpa_float64
usage=items
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
DB*_PERF_SEL_POSTZ_SAMPLES_FAILING_Z[0..15]
eqn=0..15,sum16

name=ZUnitStalled
desc=#DepthAndStencil#The percentage of GPUTime the depth buffer spends waiting for the color buffer to be ready to accept data. High figures here indicate a bottleneck in color buffer operations.
type=gpa_float64
usage=percentage
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
DB*_PERF_SEL_DB_CB_LQUAD_STALLS[0..15]
CPF_PERF_SEL_CPF_STAT_BUSY
eqn=0..15,max16,16,/,(100),*

name=ZUnitStalledCycles
desc=#DepthAndStencil#Number of GPU cycles the depth buffer spends waiting for the color buffer to be ready to accept data. Larger numbers indicate a bottleneck in color buffer operations.
type=gpa_float64
usage=cycles
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
DB*_PERF_SEL_DB_CB_LQUAD_STALLS[0..15]
eqn=0..15,max16

name=DBMemRead
desc=#DepthAndStencil#Number of bytes read from the depth buffer.
type=gpa_float64
usage=bytes
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
DB*_PERF_SEL_TILE_RD_SENDS[0..15]
DB*_PERF_SEL_QUAD_RD_32BYTE_REQS[0..15]
eqn=0..15,sum16,(256),*,16..31,sum16,(32),*,+

name=DBMemWritten
desc=#DepthAndStencil#Number of bytes written to the depth buffer.
type=gpa_float64
usage=bytes
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
DB*_PERF_SEL_TILE_WR_SENDS[0..15]
DB*_PERF_SEL_QUAD_WR_SENDS[0..15]
eqn=0..15,sum16,(32),*,16..31,sum16,(32),*,+

name=CBMemRead
desc=#ColorBuffer#Number of bytes read from the color buffer.
type=gpa_float64
usage=bytes
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
CB*_PERF_SEL_CC_MC_READ_REQUEST[0..15]
eqn=0..15,sum16,(32),*

name=CBColorAndMaskRead
desc=#ColorBuffer#Total number of bytes read from the color and mask buffers.
type=gpa_float64
usage=bytes
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
CB*_PERF_SEL_FC_MC_DCC_READ_REQUEST[0..15]
CB*_PERF_SEL_CM_MC_READ_REQUEST[0..15]
CB*_PERF_SEL_FC_MC_READ_REQUEST[0..15]
CB*_PERF_SEL_CC_MC_READ_REQUEST[0..15]
eqn=0..63,sum64,(32),*

name=CBMemWritten
desc=#ColorBuffer#Number of bytes written to the color buffer.
type=gpa_float64
usage=bytes
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
CB*_PERF_SEL_CC_MC_WRITE_REQUEST[0..15]
eqn=0..15,sum16,(32),*

name=CBColorAndMaskWritten
desc=#ColorBuffer#Total number of bytes written to the color and mask buffers.
type=gpa_float64
usage=bytes
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
CB*_PERF_SEL_FC_MC_DCC_WRITE_REQUEST[0..15]
CB*_PERF_SEL_CM_MC_WRITE_REQUEST[0..15]
CB*_PERF_SEL_FC_MC_WRITE_REQUEST[0..15]
CB*_PERF_SEL_CC_MC_WRITE_REQUEST[0..15]
eqn=0..63,sum64,(32),*

name=CBSlowPixelPct
desc=#ColorBuffer#Percentage of pixels written to the color buffer using a half-rate or quarter-rate format.
type=gpa_float64
usage=percentage
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
CB*_PERF_SEL_EXPORT_32_ABGR_QUAD_FRAGMENT[0..15]
CB*_PERF_SEL_DRAWN_QUAD_FRAGMENT[0..15]
eqn=0..15,sum16,16..31,sum16,/,(100),*,(100),min

name=CBSlowPixelCount
desc=#ColorBuffer#Number of pixels written to the color buffer using a half-rate or quarter-rate format.
type=gpa_float64
usage=items
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
CB*_PERF_SEL_EXPORT_32_ABGR_QUAD_FRAGMENT[0..15]
eqn=0..15,sum16

; Cache counters
;
; L0 data (TCP) and L1 (GL1) provide only a requests and misses counter. Scalar and Instruction caches (SQC) and L2 (GL2C) all provide a requests,
; hits, and misses counter. For L2CacheHit, we are intentionally using similar counters and formulas to those used in L0CacheHit and L1CacheHit.
; By using the "MISS" counter in all cases, we 1) have a consistent formula between L0, L1 and L2 and 2) the scheduler will ensure all the L2Cache
; hardware counters are in the same pass (this won't be guaranteed if L2CacheHit uses "HIT" and L2CacheMiss uses "MISS"). For InstCacheHit and
; ScalarCacheHit, we use HITS, MISSES and MISSES_DUPLICATE, to calculate the number of requests. This is done to ensure that the number of requests
; always equals the number of hits + misses. There are small variances in the data where sometimes the number of requests is slightly different
; than the number of hits + misses. This is just "noise", but by using hits + misses, we ensure that the data always adds up.

name=L0CacheHit
desc=#MemoryCache#The percentage of read requests that hit the data in the L0 cache. The L0 cache contains vector data, which is data that may vary in each thread across the wavefront. Each request is 128 bytes in size. Value range: 0% (no hit) to 100% (optimal).
type=gpa_float64
usage=percentage
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
TCP*_PERF_SEL_REQ[0..79]
TCP*_PERF_SEL_REQ_MISS[0..79]
eqn=(0),(1),80..159,sum80,0..79,sum80,/,-,(100),*,0..79,sum80,ifnotzero

name=L0CacheRequestCount
desc=#MemoryCache#The number of read requests made to the L0 cache. The L0 cache contains vector data, which is data that may vary in each thread across the wavefront. Each request is 128 bytes in size.
type=gpa_float64
usage=items
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
TCP*_PERF_SEL_REQ[0..79]
eqn=0..79,sum80

name=L0CacheHitCount
desc=#MemoryCache#The number of read requests which result in a cache hit from the L0 cache. The L0 cache contains vector data, which is data that may vary in each thread across the wavefront. Each request is 128 bytes in size.
type=gpa_float64
usage=items
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
TCP*_PERF_SEL_REQ[0..79]
TCP*_PERF_SEL_REQ_MISS[0..79]
eqn=0..79,sum80,80..159,sum80,-

name=L0CacheMissCount
desc=#MemoryCache#The number of read requests which result in a cache miss from the L0 cache. The L0 cache contains vector data, which is data that may vary in each thread across the wavefront. Each request is 128 bytes in size.
type=gpa_float64
usage=items
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
TCP*_PERF_SEL_REQ_MISS[0..79]
eqn=0..79,sum80

name=ScalarCacheHit
desc=#MemoryCache#The percentage of read requests made from executing shader code that hit the data in the Scalar cache. The Scalar cache contains data that does not vary in each thread across the wavefront. Each request is 64 bytes in size. Value range: 0% (no hit) to 100% (optimal).
type=gpa_float64
usage=percentage
;[DX11Gfx103]
[DX12Gfx103]
;[OGLPGfx103]
[VKGfx103]
SQ*_SQC_PERF_SEL_DCACHE_HITS[0..3]
SQ*_SQC_PERF_SEL_DCACHE_MISSES[0..3]
SQ*_SQC_PERF_SEL_DCACHE_MISSES_DUPLICATE[0..3]
eqn=(0),0..3,sum4,0..11,sum12,/,(100),*,0..11,sum12,ifnotzero

name=ScalarCacheRequestCount
desc=#MemoryCache#The number of read requests made from executing shader code to the Scalar cache. The Scalar cache contains data that does not vary in each thread across the wavefront. Each request is 64 bytes in size.
type=gpa_float64
usage=items
;[DX11Gfx103]
[DX12Gfx103]
;[OGLPGfx103]
[VKGfx103]
SQ*_SQC_PERF_SEL_DCACHE_HITS[0..3]
SQ*_SQC_PERF_SEL_DCACHE_MISSES[0..3]
SQ*_SQC_PERF_SEL_DCACHE_MISSES_DUPLICATE[0..3]
eqn=0..11,sum12

name=ScalarCacheHitCount
desc=#MemoryCache#The number of read requests made from executing shader code which result in a cache hit from the Scalar cache. The Scalar cache contains data that does not vary in each thread across the wavefront. Each request is 64 bytes in size.
type=gpa_float64
usage=items
;[DX11Gfx103]
[DX12Gfx103]
;[OGLPGfx103]
[VKGfx103]
SQ*_SQC_PERF_SEL_DCACHE_HITS[0..3]
eqn=0..3,sum4

name=ScalarCacheMissCount
desc=#MemoryCache#The number of read requests made from executing shader code which result in a cache miss from the Scalar cache. The Scalar cache contains data that does not vary in each thread across the wavefront. Each request is 64 bytes in size.
type=gpa_float64
usage=items
;[DX11Gfx103]
[DX12Gfx103]
;[OGLPGfx103]
[VKGfx103]
SQ*_SQC_PERF_SEL_DCACHE_MISSES[0..3]
SQ*_SQC_PERF_SEL_DCACHE_MISSES_DUPLICATE[0..3]
eqn=0..7,sum8

name=InstCacheHit
desc=#MemoryCache#The percentage of read requests made that hit the data in the Instruction cache. The Instruction cache supplies shader code to an executing shader. Each request is 64 bytes in size. Value range: 0% (no hit) to 100% (optimal).
type=gpa_float64
usage=percentage
;[DX11Gfx103]
[DX12Gfx103]
;[OGLPGfx103]
[VKGfx103]
SQ*_SQC_PERF_SEL_ICACHE_HITS[0..3]
SQ*_SQC_PERF_SEL_ICACHE_MISSES[0..3]
SQ*_SQC_PERF_SEL_ICACHE_MISSES_DUPLICATE[0..3]
eqn=(0),0..3,sum4,0..11,sum12,/,(100),*,0..11,sum12,ifnotzero

name=InstCacheRequestCount
desc=#MemoryCache#The number of read requests made to the Instruction cache. The Instruction cache supplies shader code to an executing shader. Each request is 64 bytes in size.
type=gpa_float64
usage=items
;[DX11Gfx103]
[DX12Gfx103]
;[OGLPGfx103]
[VKGfx103]
SQ*_SQC_PERF_SEL_ICACHE_HITS[0..3]
SQ*_SQC_PERF_SEL_ICACHE_MISSES[0..3]
SQ*_SQC_PERF_SEL_ICACHE_MISSES_DUPLICATE[0..3]
eqn=0..11,sum12

name=InstCacheHitCount
desc=#MemoryCache#The number of read requests which result in a cache hit from the Instruction cache. The Instruction cache supplies shader code to an executing shader. Each request is 64 bytes in size.
type=gpa_float64
usage=items
;[DX11Gfx103]
[DX12Gfx103]
;[OGLPGfx103]
[VKGfx103]
SQ*_SQC_PERF_SEL_ICACHE_HITS[0..3]
eqn=0..3,sum4

name=InstCacheMissCount
desc=#MemoryCache#The number of read requests which result in a cache miss from the Instruction cache. The Instruction cache supplies shader code to an executing shader. Each request is 64 bytes in size.
type=gpa_float64
usage=items
;[DX11Gfx103]
[DX12Gfx103]
;[OGLPGfx103]
[VKGfx103]
SQ*_SQC_PERF_SEL_ICACHE_MISSES[0..3]
SQ*_SQC_PERF_SEL_ICACHE_MISSES_DUPLICATE[0..3]
eqn=0..7,sum8

name=L1CacheHit
desc=#MemoryCache#The percentage of read or write requests that hit the data in the L1 cache. The L1 cache is shared across all WGPs in a single shader engine. Each request is 128 bytes in size. Value range: 0% (no hit) to 100% (optimal).
type=gpa_float64
usage=percentage
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
GL1C*_PERF_SEL_REQ[0..31]
GL1C*_PERF_SEL_REQ_MISS[0..31]
eqn=(0),(1),32..63,sum32,0..31,sum32,/,-,(100),*,0..31,sum32,ifnotzero

name=L1CacheRequestCount
desc=#MemoryCache#The number of read or write requests made to the L1 cache. The L1 cache is shared across all WGPs in a single shader engine. Each request is 128 bytes in size.
type=gpa_float64
usage=items
spm=0
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
GL1C*_PERF_SEL_REQ[0..31]
eqn=0..31,sum32

name=L1CacheHitCount
desc=#MemoryCache#The number of read or write requests which result in a cache hit from the L1 cache. The L1 cache is shared across all WGPs in a single shader engine. Each request is 128 bytes in size.
type=gpa_float64
usage=items
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
GL1C*_PERF_SEL_REQ[0..31]
GL1C*_PERF_SEL_REQ_MISS[0..31]
eqn=0..31,sum32,32..63,sum32,-

name=L1CacheMissCount
desc=#MemoryCache#The number of read or write requests which result in a cache miss from the L1 cache. The L1 cache is shared across all WGPs in a single shader engine. Each request is 128 bytes in size.
type=gpa_float64
usage=items
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
GL1C*_PERF_SEL_REQ_MISS[0..31]
eqn=0..31,sum32

name=L2CacheHit
desc=#MemoryCache#The percentage of read or write requests that hit the data in the L2 cache. The L2 cache is shared by many blocks across the GPU, including the Command Processor, Geometry Engine, all WGPs, all Render Backends, and others. Each request is 128 bytes in size. Value range: 0% (no hit) to 100% (optimal).
type=gpa_float64
usage=percentage
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
GL2C*_PERF_SEL_REQ[0..15]
GL2C*_PERF_SEL_MISS[0..15]
eqn=(0),(1),16..31,sum16,0..15,sum16,/,-,(100),*,0..15,sum16,ifnotzero

name=L2CacheMiss
desc=#MemoryCache#The percentage of read or write requests that miss the data in the L2 cache. The L2 cache is shared by many blocks across the GPU, including the Command Processor, Geometry Engine, all WGPs, all Render Backends, and others. Each request is 128 bytes in size. Value range: 0% (optimal) to 100% (all miss).
type=gpa_float64
usage=percentage
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
GL2C*_PERF_SEL_MISS[0..15]
GL2C*_PERF_SEL_REQ[0..15]
eqn=0..15,sum16,16..31,sum16,/,(100),*

name=L2CacheRequestCount
desc=#MemoryCache#The number of read or write requests made to the L2 cache. The L2 cache is shared by many blocks across the GPU, including the Command Processor, Geometry Engine, all WGPs, all Render Backends, and others. Each request is 128 bytes in size.
type=gpa_float64
usage=items
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
GL2C*_PERF_SEL_REQ[0..15]
eqn=0..15,sum16

name=L2CacheHitCount
desc=#MemoryCache#The number of read or write requests which result in a cache hit from the L2 cache. The L2 cache is shared by many blocks across the GPU, including the Command Processor, Geometry Engine, all WGPs, all Render Backends, and others. Each request is 128 bytes in size.
type=gpa_float64
usage=items
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
GL2C*_PERF_SEL_REQ[0..15]
GL2C*_PERF_SEL_MISS[0..15]
eqn=0..15,sum16,16..31,sum16,-

name=L2CacheMissCount
desc=#MemoryCache#The number of read or write requests which result in a cache miss from the L2 cache. The L2 cache is shared by many blocks across the GPU, including the Command Processor, Geometry Engine, all WGPs, all Render Backends, and others. Each request is 128 bytes in size.
type=gpa_float64
usage=items
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
GL2C*_PERF_SEL_MISS[0..15]
eqn=0..15,sum16

name=L0TagConflictReadStalledCycles
desc=#MemoryCache#The number of cycles read operations from the L0 cache are stalled due to tag conflicts.
type=gpa_float64
usage=items
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
TCP*_PERF_SEL_READ_TAGCONFLICT_STALL[0..79]
eqn=0..79,max80

name=L0TagConflictWriteStalledCycles
desc=#MemoryCache#The number of cycles write operations to the L0 cache are stalled due to tag conflicts.
type=gpa_float64
usage=items
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
TCP*_PERF_SEL_WRITE_TAGCONFLICT_STALL[0..79]
eqn=0..79,max80

name=L0TagConflictAtomicStalledCycles
desc=#MemoryCache#The number of cycles atomic operations on the L0 cache are stalled due to tag conflicts.
type=gpa_float64
usage=items
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
TCP*_PERF_SEL_ATOMIC_TAGCONFLICT_STALL[0..79]
eqn=0..79,max80

name=FetchSize
desc=#GlobalMemory#The total bytes fetched from the video memory. This is measured with all extra fetches and any cache or memory effects taken into account.
type=gpa_float64
usage=bytes
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
GL2C*_PERF_SEL_EA_RDREQ_32B[0..15]
GL2C*_PERF_SEL_EA_RDREQ_64B[0..15]
GL2C*_PERF_SEL_EA_RDREQ_96B[0..15]
GL2C*_PERF_SEL_EA_RDREQ_128B[0..15]
eqn=0..15,sum16,(32),*,16..31,sum16,(64),*,32..47,sum16,(96),*,48..63,sum16,(128),*,sum4

name=WriteSize
desc=#GlobalMemory#The total bytes written to the video memory. This is measured with all extra fetches and any cache or memory effects taken into account.
type=gpa_float64
usage=bytes
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
GL2C*_PERF_SEL_MC_WRREQ[0..15]
GL2C*_PERF_SEL_EA_WRREQ_64B[0..15]
eqn=0..15,sum16,16..31,sum16,-,(32),*,16..31,sum16,(64),*,+

name=MemUnitBusy
desc=#GlobalMemory#The percentage of GPUTime the memory unit is active. The result includes the stall time (MemUnitStalled). This is measured with all extra fetches and writes and any cache or memory effects taken into account. Value range: 0% to 100% (fetch-bound).
type=gpa_float64
usage=percentage
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
TA*_PERF_SEL_TA_BUSY[0..79]
CPF_PERF_SEL_CPF_STAT_BUSY
eqn=0..79,max80,80,/,(100),*

name=MemUnitBusyCycles
desc=#GlobalMemory#Number of GPU cycles the memory unit is active. The result includes the stall time (MemUnitStalledCycles). This is measured with all extra fetches and writes and any cache or memory effects taken into account.
type=gpa_float64
usage=cycles
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
TA*_PERF_SEL_TA_BUSY[0..79]
eqn=0..79,max80

name=MemUnitStalled
desc=#GlobalMemory#The percentage of GPUTime the memory unit is stalled. Try reducing the number or size of fetches and writes if possible. Value range: 0% (optimal) to 100% (bad).
type=gpa_float64
usage=percentage
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
TCP*_PERF_SEL_TCP_TA_REQ_STALL[0..79]
CPF_PERF_SEL_CPF_STAT_BUSY
eqn=0..79,max80,80,/,(100),*

name=MemUnitStalledCycles
desc=#GlobalMemory#Number of GPU cycles the memory unit is stalled.
type=gpa_float64
usage=cycles
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
TCP*_PERF_SEL_TCP_TA_REQ_STALL[0..79]
eqn=0..79,max80

name=WriteUnitStalled
desc=#GlobalMemory#The percentage of GPUTime the Write unit is stalled. Value range: 0% to 100% (bad).
type=gpa_float64
usage=percentage
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
GL2C*_PERF_SEL_MC_WRREQ_STALL[0..15]
CPF_PERF_SEL_CPF_STAT_BUSY
eqn=0..15,max16,16,/,(100),*

name=WriteUnitStalledCycles
desc=#GlobalMemory#Number of GPU cycles the Write unit is stalled.
type=gpa_float64
usage=cycles
[DX11Gfx103]
[DX12Gfx103]
[OGLPGfx103]
[VKGfx103]
GL2C*_PERF_SEL_MC_WRREQ_STALL[0..15]
eqn=0..15,max16

name=LocalVidMemBytes
desc=#GlobalMemory#Number of bytes read from or written to the Infinity Cache (if available) or local video memory
type=gpa_float64
usage=bytes
spm=0
[DX12Gfx103]
;[OGLPGfx103]
[VKGfx103]
GCEA*_PERF_SEL_SARB_DRAM_SIZED_REQUESTS[0..15]
eqn=0..15,sum16,(32),*

name=PcieBytes
desc=#GlobalMemory#Number of bytes sent and received over the PCIe bus
type=gpa_float64
usage=bytes
spm=0
[DX12Gfx103]
;[OGLPGfx103]
[VKGfx103]
GCEA*_PERF_SEL_SARB_IO_SIZED_REQUESTS[0..15]
eqn=0..15,sum16,(32),*

name=RayTriTests
desc=#RayTracing#The number of ray triangle intersection tests.
type=gpa_float64
usage=items
[DX12Gfx103]
;[OGLPGfx103]
[VKGfx103]
TD*_PERF_SEL_RAY_TRACING_BVH4_TRI_NODE[0..79]
eqn=0..79,sum80

name=RayBoxTests
desc=#RayTracing#The number of ray box intersection tests.
type=gpa_float64
usage=items
[DX12Gfx103]
;[OGLPGfx103]
[VKGfx103]
TD*_PERF_SEL_RAY_TRACING_BVH4_FP16_BOX_NODE[0..79]
TD*_PERF_SEL_RAY_TRACING_BVH4_FP32_BOX_NODE[0..79]
eqn=0..79,sum80,80..159,sum80,sum2

name=TotalRayTests
desc=#RayTracing#Total number of ray intersection tests, includes both box and triangle intersections.
type=gpa_float64
usage=items
[DX12Gfx103]
;[OGLPGfx103]
[VKGfx103]
TD*_PERF_SEL_RAY_TRACING_BVH4_TRI_NODE[0..79]
TD*_PERF_SEL_RAY_TRACING_BVH4_FP16_BOX_NODE[0..79]
TD*_PERF_SEL_RAY_TRACING_BVH4_FP32_BOX_NODE[0..79]
eqn=0..79,sum80,80..159,sum80,sum2,160..239,sum80,sum2

name=RayTestsPerWave
desc=#RayTracing#The number of ray intersection tests per wave.
type=gpa_float64
usage=items
[DX12Gfx103]
;[OGLPGfx103]
[VKGfx103]
TD*_PERF_SEL_TOTAL_NUM_RAY_TRACING_BVH4_INSTR[0..79]
eqn=0..79,sum80

name=WaveOccupancyPct
desc=#WaveDistribution#The percentage of the maximum wavefront occupancy that is currently being used.
type=gpa_float64
usage=percentage
spm=1
discrete=0
[DX12Gfx103]
SQ*_PERF_SEL_LEVEL_WAVES[0..3]
eqn=0..3,sum4,MAX_WAVES,/,(100),*

name=LSHSLimitedByVgpr
desc=#WaveOccupancyLimiters#The percentage of LS and HS wave scheduling requests that are limited by VGPR availability.
type=gpa_float64
usage=percentage
spm=1
discrete=0
[DX12Gfx103]
SPI*_PERF_RA_VGPR_SIMD_FULL_LS[0..3]
SPI*_PERF_LS_BUSY[0..3]
SPI*_PERF_RA_VGPR_SIMD_FULL_HS[0..3]
SPI*_PERF_HS_BUSY[0..3]
eqn=0..3,sum4,4..7,max4,/,8..11,sum4,12..15,max4,/,max,NUM_SIMDS,/,(100),*

name=LSHSLimitedByLds
desc=#WaveOccupancyLimiters#The percentage of LS and HS wave scheduling requests that are limited by LDS availability.
type=gpa_float64
usage=percentage
spm=1
discrete=0
[DX12Gfx103]
SPI*_PERF_RA_LDS_CU_FULL_LS[0..3]
SPI*_PERF_LS_BUSY[0..3]
SPI*_PERF_RA_LDS_CU_FULL_HS[0..3]
SPI*_PERF_HS_BUSY[0..3]
eqn=0..3,sum4,4..7,max4,/,8..11,sum4,12..15,max4,/,max,NUM_CUS,/,(100),*

name=LSHSLimitedByScratch
desc=#WaveOccupancyLimiters#The percentage of LS and HS wave scheduling requests that are limited by scratch space availability.
type=gpa_float64
usage=percentage
spm=1
discrete=0
[DX12Gfx103]
SPI*_PERF_RA_TMP_STALL_LS[0..3]
SPI*_PERF_LS_BUSY[0..3]
SPI*_PERF_RA_TMP_STALL_HS[0..3]
SPI*_PERF_HS_BUSY[0..3]
eqn=0..3,sum4,4..7,max4,/,8..11,sum4,12..15,max4,/,max,(100),*

name=HSLimitedByBarriers
desc=#WaveOccupancyLimiters#The percentage of HS wave scheduling requests that are limited by barriers.
type=gpa_float64
usage=percentage
spm=1
discrete=0
[DX12Gfx103]
SPI*_PERF_RA_BAR_CU_FULL_HS[0..3]
SPI*_PERF_HS_BUSY[0..3]
eqn=0..3,sum4,4..7,max4,/,NUM_CUS,/,(100),*

name=ESGSLimitedByVgpr
desc=#WaveOccupancyLimiters#The percentage of ES and GS wave scheduling requests that are limited by VGPR availability.
type=gpa_float64
usage=percentage
spm=1
discrete=0
[DX12Gfx103]
SPI*_PERF_RA_VGPR_SIMD_FULL_ES[0..3]
SPI*_PERF_ES_BUSY[0..3]
SPI*_PERF_RA_VGPR_SIMD_FULL_GS[0..3]
SPI*_PERF_GS_BUSY[0..3]
eqn=0..3,sum4,4..7,max4,/,8..11,sum4,12..15,max4,/,max,NUM_SIMDS,/,(100),*

name=ESGSLimitedByLds
desc=#WaveOccupancyLimiters#The percentage of ES and GS wave scheduling requests that are limited by LDS availability.
type=gpa_float64
usage=percentage
spm=1
discrete=0
[DX12Gfx103]
SPI*_PERF_RA_LDS_CU_FULL_ES[0..3]
SPI*_PERF_ES_BUSY[0..3]
SPI*_PERF_RA_LDS_CU_FULL_GS[0..3]
SPI*_PERF_GS_BUSY[0..3]
eqn=0..3,sum4,4..7,max4,/,8..11,sum4,12..15,max4,/,max,NUM_CUS,/,(100),*

name=ESGSLimitedByScratch
desc=#WaveOccupancyLimiters#The percentage of ES and GS wave scheduling requests that are limited by scratch space availability.
type=gpa_float64
usage=percentage
spm=1
discrete=0
[DX12Gfx103]
SPI*_PERF_RA_TMP_STALL_ES[0..3]
SPI*_PERF_ES_BUSY[0..3]
SPI*_PERF_RA_TMP_STALL_GS[0..3]
SPI*_PERF_GS_BUSY[0..3]
eqn=0..3,sum4,4..7,max4,/,8..11,sum4,12..15,max4,/,max,(100),*

name=VSLimitedByVgpr
desc=#WaveOccupancyLimiters#The percentage of VS wave scheduling requests that are limited by VGPR availability.
type=gpa_float64
usage=percentage
spm=1
discrete=0
[DX12Gfx103]
SPI*_PERF_RA_VGPR_SIMD_FULL_VS[0..3]
SPI*_PERF_VS_BUSY[0..3]
eqn=0..3,sum4,4..7,max4,/,NUM_SIMDS,/,(100),*

name=VSLimitedByScratch
desc=#WaveOccupancyLimiters#The percentage of VS wave scheduling requests that are limited by scractch space availability.
type=gpa_float64
usage=percentage
spm=1
discrete=0
[DX12Gfx103]
SPI*_PERF_RA_TMP_STALL_VS[0..3]
SPI*_PERF_VS_BUSY[0..3]
eqn=0..3,sum4,4..7,max4,/,(100),*

name=VSLimitedByExport
desc=#WaveOccupancyLimiters#The percentage of cycles that VS Waves are stalled due to export space availability.
type=gpa_float64
usage=percentage
spm=1
discrete=0
[DX12Gfx103]
SX*_PERF_SEL_SH_POS_STALL[0..3]
GE2_DIST_GE_DIST_PC_SPACE_ZERO
SX*_PERF_SEL_CLOCK[0..3]
eqn=0..3,max4,4,max,5..8,max4,/,(100),*

name=PSLimitedByLds
desc=#WaveOccupancyLimiters#The percentage of PS wave scheduling requests that are limited by LDS availability.
type=gpa_float64
usage=percentage
spm=1
discrete=0
[DX12Gfx103]
SPI*_PERF_RA_LDS_CU_FULL_PS[0..3]
SPI*_PERF_PS0_BUSY[0..3]
SPI*_PERF_PS1_BUSY[0..3]
SPI*_PERF_PS2_BUSY[0..3]
SPI*_PERF_PS3_BUSY[0..3]
eqn=0..3,sum4,4..19,max16,/,NUM_CUS,/,(100),*

name=PSLimitedByVgpr
desc=#WaveOccupancyLimiters#The percentage of PS wave scheduling requests that are limited by VGPR availability.
type=gpa_float64
usage=percentage
spm=1
discrete=0
[DX12Gfx103]
SPI*_PERF_RA_VGPR_SIMD_FULL_PS[0..3]
SPI*_PERF_PS0_BUSY[0..3]
SPI*_PERF_PS1_BUSY[0..3]
SPI*_PERF_PS2_BUSY[0..3]
SPI*_PERF_PS3_BUSY[0..3]
eqn=0..3,sum4,4..19,max16,/,NUM_SIMDS,/,(100),*

name=PSLimitedByScratch
desc=#WaveOccupancyLimiters#The percentage of PS wave scheduling requests that are limited by scratch space availability.
type=gpa_float64
usage=percentage
spm=1
discrete=0
[DX12Gfx103]
SPI*_PERF_RA_TMP_STALL_PS[0..3]
SPI*_PERF_PS0_BUSY[0..3]
SPI*_PERF_PS1_BUSY[0..3]
SPI*_PERF_PS2_BUSY[0..3]
SPI*_PERF_PS3_BUSY[0..3]
eqn=0..3,sum4,4..19,max16,/,(100),*

name=CSLimitedByLds
desc=#WaveOccupancyLimiters#The percentage of CS wave scheduling requests that are limited by LDS availability.
type=gpa_float64
usage=percentage
spm=1
discrete=0
[DX12Gfx103]
SPI*_PERF_RA_LDS_CU_FULL_CSG[0..3]
SPI*_PERF_CSGN_BUSY[0..3]
eqn=0..3,sum4,4..7,max4,/,NUM_CUS,/,(100),*

name=CSLimitedByVgpr
desc=#WaveOccupancyLimiters#The percentage of CS wave scheduling requests that are limited by VGPR availability.
type=gpa_float64
usage=percentage
spm=1
discrete=0
[DX12Gfx103]
SPI*_PERF_RA_VGPR_SIMD_FULL_CSG[0..3]
SPI*_PERF_CSGN_BUSY[0..3]
eqn=0..3,sum4,4..7,max4,/,NUM_SIMDS,/,(100),*

name=CSLimitedByScratch
desc=#WaveOccupancyLimiters#The percentage of CS wave scheduling requests that are limited by scratch space availability.
type=gpa_float64
usage=percentage
spm=1
discrete=0
[DX12Gfx103]
SPI*_PERF_RA_TMP_STALL_CSG[0..3]
SPI*_PERF_CSGN_BUSY[0..3]
eqn=0..3,sum4,4..7,max4,/,(100),*

name=CSLimitedByBarriers
desc=#WaveOccupancyLimiters#The percentage of CS wave scheduling requests that are limited by barriers.
type=gpa_float64
usage=percentage
spm=1
discrete=0
[DX12Gfx103]
SPI*_PERF_RA_BAR_CU_FULL_CSG[0..3]
SPI*_PERF_CSGN_BUSY[0..3]
eqn=0..3,sum4,4..7,max4,/,NUM_CUS,/,(100),*

name=CSLimitedByThreadGroupLimit
desc=#WaveOccupancyLimiters#The percentage of CS wave scheduling requests that are limited by the thread group limit.
type=gpa_float64
usage=percentage
spm=1
discrete=0
[DX12Gfx103]
SPI*_PERF_RA_TGLIM_CU_FULL_CSG[0..3]
SPI*_PERF_CSGN_BUSY[0..3]
eqn=0..3,sum4,4..7,max4,/,NUM_CUS,/,(100),*

